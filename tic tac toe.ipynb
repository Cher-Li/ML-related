{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#\n",
        "# Game Master AI: Learning to Win Tic-Tac-Toe with Q-Learning\n",
        "#\n",
        "# This script contains all the code needed to:\n",
        "# 1. Create a Tic-Tac-Toe game environment.\n",
        "# 2. Build an AI \"Agent\" that uses Q-learning to make decisions.\n",
        "# 3. Train the AI by making it play against itself thousands of times.\n",
        "# 4. Allow a human player to challenge the fully trained AI.\n",
        "#\n",
        "# To run this, save it as a Python file (e.g., `tictactoe_ai.py`) and\n",
        "# run it from your terminal: `python tictactoe_ai.py`\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import random\n",
        "import time\n",
        "import json # To save and load the AI's \"brain\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Part 1: The Tic-Tac-Toe Game Environment\n",
        "# ==============================================================================\n",
        "class TicTacToe:\n",
        "    \"\"\"\n",
        "    This class represents the Tic-Tac-Toe game. It doesn't know about the AI;\n",
        "    it only knows the rules of the game, how to make moves, and how to check\n",
        "    for a winner.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # The board is a list of 9 strings. '_' means the space is empty.\n",
        "        self.board = ['_'] * 9\n",
        "\n",
        "    def print_board(self):\n",
        "        \"\"\"Prints the current game board to the console in a readable format.\"\"\"\n",
        "        print(\"\")\n",
        "        # We use an f-string to format the board nicely.\n",
        "        board_str = (\n",
        "            f\" {self.board[0]} | {self.board[1]} | {self.board[2]} \\n\"\n",
        "            \"-----------\\n\"\n",
        "            f\" {self.board[3]} | {self.board[4]} | {self.board[5]} \\n\"\n",
        "            \"-----------\\n\"\n",
        "            f\" {self.board[6]} | {self.board[7]} | {self.board[8]} \\n\"\n",
        "        )\n",
        "        print(board_str)\n",
        "\n",
        "    def get_available_moves(self):\n",
        "        \"\"\"Returns a list of the indices of all empty spots on the board.\"\"\"\n",
        "        return [i for i, spot in enumerate(self.board) if spot == '_']\n",
        "\n",
        "    def make_move(self, position, player_symbol):\n",
        "        \"\"\"Places a player's symbol on the board if the move is valid.\"\"\"\n",
        "        if self.board[position] == '_':\n",
        "            self.board[position] = player_symbol\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self, player_symbol):\n",
        "        \"\"\"Checks if the specified player has won the game.\"\"\"\n",
        "        # All possible winning combinations (rows, columns, diagonals)\n",
        "        win_conditions = [\n",
        "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n",
        "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n",
        "            (0, 4, 8), (2, 4, 6)             # Diagonals\n",
        "        ]\n",
        "        for condition in win_conditions:\n",
        "            # Check if all spots in a winning condition are filled by the player's symbol\n",
        "            if all(self.board[i] == player_symbol for i in condition):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_draw(self):\n",
        "        \"\"\"Checks if the game is a draw (i.e., the board is full).\"\"\"\n",
        "        return '_' not in self.board\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the board to an empty state for a new game.\"\"\"\n",
        "        self.board = ['_'] * 9\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        Returns the current state of the board as a hashable tuple.\n",
        "        This is crucial because dictionary keys (which we use for the Q-table)\n",
        "        must be hashable, and lists are not.\n",
        "        \"\"\"\n",
        "        return tuple(self.board)\n",
        "\n",
        "# ==============================================================================\n",
        "# Part 2: The AI Agent using Q-Learning\n",
        "# ==============================================================================\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    This class represents the AI player. It uses a Q-learning algorithm\n",
        "    to learn the best move for any given game state.\n",
        "    \"\"\"\n",
        "    def __init__(self, symbol, learning_rate=0.1, discount_factor=0.9, epsilon=0.9):\n",
        "        self.symbol = symbol\n",
        "        self.learning_rate = learning_rate      # alpha: How much we update Q-values based on new info.\n",
        "        self.discount_factor = discount_factor  # gamma: How much we value future rewards.\n",
        "        self.epsilon = epsilon                  # The probability of choosing a random move (exploration).\n",
        "        self.q_table = {}                       # The AI's \"brain\". Maps (state) -> {action: q_value}.\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        \"\"\"Safely gets the Q-value for a state-action pair, defaulting to 0.0 if not found.\"\"\"\n",
        "        return self.q_table.get(state, {}).get(action, 0.0)\n",
        "\n",
        "    def choose_action(self, state, available_moves):\n",
        "        \"\"\"\n",
        "        Chooses an action using an epsilon-greedy strategy.\n",
        "        - With probability epsilon, it explores by choosing a random move.\n",
        "        - Otherwise, it exploits its current knowledge by choosing the best known move.\n",
        "        \"\"\"\n",
        "        if not available_moves:\n",
        "            return None\n",
        "\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            # Exploration: choose a random available move.\n",
        "            return random.choice(available_moves)\n",
        "        else:\n",
        "            # Exploitation: choose the best move based on Q-values.\n",
        "            q_values = {move: self.get_q_value(state, move) for move in available_moves}\n",
        "            max_q = max(q_values.values())\n",
        "            # If multiple moves have the same max Q-value, choose one randomly.\n",
        "            best_moves = [move for move, q in q_values.items() if q == max_q]\n",
        "            return random.choice(best_moves)\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, next_available_moves):\n",
        "        \"\"\"\n",
        "        Updates the Q-table using the Bellman equation for Q-learning.\n",
        "        Q(s, a) = Q(s, a) + alpha * [R + gamma * max(Q(s', a')) - Q(s, a)]\n",
        "        \"\"\"\n",
        "        # Get the best Q-value for the next state to calculate the target value.\n",
        "        max_next_q = 0.0\n",
        "        if next_available_moves:\n",
        "            max_next_q = max([self.get_q_value(next_state, move) for move in next_available_moves])\n",
        "\n",
        "        # Get the current Q-value for the state-action pair.\n",
        "        old_q_value = self.get_q_value(state, action)\n",
        "\n",
        "        # Calculate the new Q-value using the formula.\n",
        "        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q - old_q_value)\n",
        "\n",
        "        # Update the Q-table with the new value.\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = {}\n",
        "        self.q_table[state][action] = new_q_value\n",
        "\n",
        "    def save_q_table(self, file_path):\n",
        "        \"\"\"Saves the Q-table to a file.\"\"\"\n",
        "        # We need to convert tuple keys to strings to save as JSON\n",
        "        string_keyed_q_table = {str(k): v for k, v in self.q_table.items()}\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(string_keyed_q_table, f)\n",
        "        print(f\"AI brain saved to {file_path}\")\n",
        "\n",
        "    def load_q_table(self, file_path):\n",
        "        \"\"\"Loads the Q-table from a file.\"\"\"\n",
        "        with open(file_path, 'r') as f:\n",
        "            string_keyed_q_table = json.load(f)\n",
        "            # Convert string keys back to tuples\n",
        "            self.q_table = {eval(k): v for k, v in string_keyed_q_table.items()}\n",
        "        print(f\"AI brain loaded from {file_path}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Part 3: The Training Process\n",
        "# ==============================================================================\n",
        "def train(agent1, agent2, game, episodes=50000):\n",
        "    \"\"\"\n",
        "    Trains two AI agents by making them play against each other repeatedly.\n",
        "    \"\"\"\n",
        "    print(f\"ü§ñ Starting AI self-play training for {episodes} games...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        game.reset()\n",
        "        current_agent = agent1\n",
        "        other_agent = agent2\n",
        "\n",
        "        while True:\n",
        "            state = game.get_state()\n",
        "            available_moves = game.get_available_moves()\n",
        "\n",
        "            if not available_moves: # Game is a draw\n",
        "                # Last move led to a draw, give a small reward to the agent who made it\n",
        "                current_agent.update_q_table(prev_state, prev_action, 1, state, [])\n",
        "                break\n",
        "\n",
        "            action = current_agent.choose_action(state, available_moves)\n",
        "            game.make_move(action, current_agent.symbol)\n",
        "            next_state = game.get_state()\n",
        "            next_available_moves = game.get_available_moves()\n",
        "\n",
        "            # Store previous state and action for rewarding later\n",
        "            prev_state, prev_action = state, action\n",
        "\n",
        "            if game.check_winner(current_agent.symbol):\n",
        "                # The current agent won! Give it a high reward.\n",
        "                current_agent.update_q_table(state, action, 10, next_state, [])\n",
        "                # The other agent made a move that led to its loss. Give it a penalty.\n",
        "                other_agent.update_q_table(last_other_agent_state, last_other_agent_action, -10, next_state, [])\n",
        "                break\n",
        "\n",
        "            # Store the state and action of the other agent for potential penalty later\n",
        "            last_other_agent_state = state\n",
        "            last_other_agent_action = action\n",
        "\n",
        "            # Switch turns\n",
        "            current_agent, other_agent = other_agent, current_agent\n",
        "\n",
        "        # Decay epsilon over time to shift from exploration to exploitation\n",
        "        if agent1.epsilon > 0.1: agent1.epsilon -= (0.8 / episodes)\n",
        "        if agent2.epsilon > 0.1: agent2.epsilon -= (0.8 / episodes)\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"‚úÖ Training complete in {duration:.2f} seconds.\")\n",
        "    # Set epsilon to 0 so the agents only use their learned strategy\n",
        "    agent1.epsilon = 0\n",
        "    agent2.epsilon = 0\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Part 4: Playing against the AI\n",
        "# ==============================================================================\n",
        "def play_vs_ai(ai_agent, game):\n",
        "    \"\"\"\n",
        "    The main game loop for a human to play against the trained AI.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Let's Play Tic-Tac-Toe! ---\")\n",
        "    print(\"You are 'X'. The AI is 'O'.\")\n",
        "    game.reset()\n",
        "\n",
        "    while True:\n",
        "        # Human Player's Turn\n",
        "        game.print_board()\n",
        "        try:\n",
        "            move = int(input(\"Enter your move (a number from 0 to 8): \"))\n",
        "            if move not in game.get_available_moves():\n",
        "                print(\"‚ùå Invalid move. That spot is taken or out of bounds. Try again.\")\n",
        "                continue\n",
        "        except ValueError:\n",
        "            print(\"‚ùå Invalid input. Please enter a number between 0 and 8.\")\n",
        "            continue\n",
        "\n",
        "        game.make_move(move, 'X')\n",
        "\n",
        "        if game.check_winner('X'):\n",
        "            game.print_board()\n",
        "            print(\"üéâ Congratulations! You won!\")\n",
        "            break\n",
        "        elif game.is_draw():\n",
        "            game.print_board()\n",
        "            print(\"ü§ù It's a draw!\")\n",
        "            break\n",
        "\n",
        "        # AI Agent's Turn\n",
        "        print(\"AI is thinking...\")\n",
        "        time.sleep(0.5)\n",
        "        state = game.get_state()\n",
        "        available_moves = game.get_available_moves()\n",
        "        ai_move = ai_agent.choose_action(state, available_moves)\n",
        "        game.make_move(ai_move, ai_agent.symbol)\n",
        "        print(f\"AI chose to move to spot {ai_move}.\")\n",
        "\n",
        "        if game.check_winner(ai_agent.symbol):\n",
        "            game.print_board()\n",
        "            print(\"üòû The AI wins! It has been trained well.\")\n",
        "            break\n",
        "        elif game.is_draw():\n",
        "            game.print_board()\n",
        "            print(\"ü§ù It's a draw!\")\n",
        "            break\n",
        "\n",
        "# ==============================================================================\n",
        "# Main Execution Block\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initialize the game environment and AI agent for the opponent ('O')\n",
        "    tic_tac_toe_game = TicTacToe()\n",
        "    agent_o = Agent(symbol='O')\n",
        "    q_table_file = 'tictactoe_q_table.json'\n",
        "\n",
        "    # Check if a pre-trained AI brain exists\n",
        "    try:\n",
        "        agent_o.load_q_table(q_table_file)\n",
        "        agent_o.epsilon = 0 # Ensure it's in exploitation mode\n",
        "        print(\"Loaded pre-trained AI. Ready to play!\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"No pre-trained AI found. Starting a new training session.\")\n",
        "        # Create a second agent to train against\n",
        "        agent_x_trainer = Agent(symbol='X')\n",
        "\n",
        "        # Train the agents\n",
        "        train(agent_x_trainer, agent_o, tic_tac_toe_game, episodes=50000)\n",
        "\n",
        "        # Save the trained brain for future use\n",
        "        agent_o.save_q_table(q_table_file)\n",
        "\n",
        "    # Start the game against the trained AI\n",
        "    play_vs_ai(agent_o, tic_tac_toe_game)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No pre-trained AI found. Starting a new training session.\n",
            "ü§ñ Starting AI self-play training for 50000 games...\n",
            "‚úÖ Training complete in 5.82 seconds.\n",
            "AI brain saved to tictactoe_q_table.json\n",
            "\n",
            "--- Let's Play Tic-Tac-Toe! ---\n",
            "You are 'X'. The AI is 'O'.\n",
            "\n",
            " _ | _ | _ \n",
            "-----------\n",
            " _ | _ | _ \n",
            "-----------\n",
            " _ | _ | _ \n",
            "\n",
            "Enter your move (a number from 0 to 8): 0\n",
            "AI is thinking...\n",
            "AI chose to move to spot 7.\n",
            "\n",
            " X | _ | _ \n",
            "-----------\n",
            " _ | _ | _ \n",
            "-----------\n",
            " _ | O | _ \n",
            "\n",
            "Enter your move (a number from 0 to 8): 4\n",
            "AI is thinking...\n",
            "AI chose to move to spot 8.\n",
            "\n",
            " X | _ | _ \n",
            "-----------\n",
            " _ | X | _ \n",
            "-----------\n",
            " _ | O | O \n",
            "\n",
            "Enter your move (a number from 0 to 8): 6\n",
            "AI is thinking...\n",
            "AI chose to move to spot 1.\n",
            "\n",
            " X | O | _ \n",
            "-----------\n",
            " _ | X | _ \n",
            "-----------\n",
            " X | O | O \n",
            "\n",
            "Enter your move (a number from 0 to 8): 2\n",
            "\n",
            " X | O | X \n",
            "-----------\n",
            " _ | X | _ \n",
            "-----------\n",
            " X | O | O \n",
            "\n",
            "üéâ Congratulations! You won!\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNSuFJfZExlb",
        "outputId": "c3f18dd5-117c-4d0c-9814-197fd88eacbd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}